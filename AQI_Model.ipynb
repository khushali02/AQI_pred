{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"air_pollution_data.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "# Assuming 'dt' is the timestamp column\n",
    "df['dt'] = pd.to_datetime(df['dt'])\n",
    "df.set_index('dt', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.1715 - val_loss: 0.1271\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1258 - val_loss: 0.1242\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1258 - val_loss: 0.1237\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1241 - val_loss: 0.1234\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1243 - val_loss: 0.1230\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1234 - val_loss: 0.1227\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1241 - val_loss: 0.1225\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1241 - val_loss: 0.1223\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1234 - val_loss: 0.1220\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1225 - val_loss: 0.1216\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1235 - val_loss: 0.1216\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1218 - val_loss: 0.1215\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1217 - val_loss: 0.1213\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1230 - val_loss: 0.1213\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1235 - val_loss: 0.1211\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1224 - val_loss: 0.1211\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1209 - val_loss: 0.1210\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1211 - val_loss: 0.1210\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1214 - val_loss: 0.1210\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1227 - val_loss: 0.1210\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1210 - val_loss: 0.1210\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1234 - val_loss: 0.1210\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1218 - val_loss: 0.1210\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1205 - val_loss: 0.1210\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1222 - val_loss: 0.1210\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1201 - val_loss: 0.1210\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1219 - val_loss: 0.1210\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1230 - val_loss: 0.1210\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1204 - val_loss: 0.1210\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1194 - val_loss: 0.1210\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1205 - val_loss: 0.1210\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1209 - val_loss: 0.1210\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1216 - val_loss: 0.1210\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1222 - val_loss: 0.1210\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1219 - val_loss: 0.1210\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1225 - val_loss: 0.1210\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1222 - val_loss: 0.1210\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1206 - val_loss: 0.1210\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1216 - val_loss: 0.1210\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1221 - val_loss: 0.1210\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1220 - val_loss: 0.1210\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1213 - val_loss: 0.1210\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1211 - val_loss: 0.1210\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1222 - val_loss: 0.1210\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1223 - val_loss: 0.1210\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1207 - val_loss: 0.1210\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1222 - val_loss: 0.1210\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1210 - val_loss: 0.1210\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1238 - val_loss: 0.1210\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1216 - val_loss: 0.1210\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1190 \n",
      "Mean Squared Error: 0.12096066772937775\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the sequence length\n",
    "sequence_length = 10\n",
    "\n",
    "# Create overlapping sequences and corresponding target sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length-1])  # Target is the last element of each sequence\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create input sequences and corresponding target sequences\n",
    "X, y = create_sequences(df.values, sequence_length)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X.reshape(-1, sequence_length * X.shape[2])).reshape(X.shape)\n",
    "y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(units=50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215, 1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictions: (215, 1)\n",
      "Shape of original data (X): (1072, 10, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of predictions:\", predictions.shape)\n",
    "print(\"Shape of original data (X):\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted_AQI\n",
      "0              3\n",
      "1              2\n",
      "2              4\n",
      "3              1\n",
      "4              3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reshape the predictions array to match the original number of samples\n",
    "predictions_reshaped = predictions.reshape(-1, 1)\n",
    "\n",
    "# Create a DataFrame with one column named \"Predicted_AQI\" containing the reshaped predictions\n",
    "predictions_df = (pd.DataFrame(predictions_reshaped, columns=['Predicted_AQI']).multiply(10)).astype(int)\n",
    "\n",
    "# Print the first few rows of the predictions DataFrame\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Original AQI: 2\n",
      "   Predicted_AQI\n",
      "0              3\n",
      "1              2\n",
      "2              4\n",
      "3              1\n",
      "4              3\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum predicted AQI value\n",
    "max_original_aqi = df['main_aqi'].max()\n",
    "\n",
    "print(\"Max Original AQI:\", max_original_aqi)\n",
    "\n",
    "\n",
    "# Scale up the predicted AQI values\n",
    "scaled_predictions = predictions_df['Predicted_AQI']* max_original_aqi \n",
    "\n",
    "# Round the scaled predictions and convert them to integers\n",
    "predictions_df['Predicted_AQI'] = predictions_df['Predicted_AQI'].apply(lambda x: max(1, int(round(x))))\n",
    "\n",
    "# Print the first few rows of the updated predictions DataFrame\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Predicted_AQI\n",
      "0               3\n",
      "1               2\n",
      "2               4\n",
      "3               1\n",
      "4               3\n",
      "5               4\n",
      "6               5\n",
      "7               2\n",
      "8               4\n",
      "9               2\n",
      "10              2\n",
      "11              2\n",
      "12              3\n",
      "13              3\n",
      "14              6\n",
      "15              4\n",
      "16              5\n",
      "17              2\n",
      "18              2\n",
      "19              5\n",
      "20              5\n",
      "21              4\n",
      "22              3\n",
      "23              1\n",
      "24              4\n",
      "25              4\n",
      "26              2\n",
      "27              2\n",
      "28              4\n",
      "29              3\n",
      "30              6\n",
      "31              5\n",
      "32              3\n",
      "33              3\n",
      "34              5\n",
      "35              2\n",
      "36              2\n",
      "37              2\n",
      "38              3\n",
      "39              4\n",
      "40              3\n",
      "41              3\n",
      "42              2\n",
      "43              3\n",
      "44              2\n",
      "45              3\n",
      "46              4\n",
      "47              4\n",
      "48              5\n",
      "49              3\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<2,>=1.18 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (1.12.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.4 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six in c:\\users\\nikit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from statsmodels.tsa.statespace.sarimax import SARIMAX\\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\\n\\n# Assuming you have a time series dataset stored in a DataFrame called 'data'\\n# with a datetime index\\n# Example:\\n# data = pd.read_csv('your_data.csv', index_col='Date', parse_dates=True)\\n\\n# 1. Preprocess the Data\\n# No missing values handling for simplicity\\n# Example: data.dropna(inplace=True)\\n\\n# Assuming 'AQI' is the column you want to use as the endogenous variable\\nendog = train['main_aqi']\\n\\n# Define SARIMA parameters\\norder = (1, 1, 1)  # Non-seasonal parameters\\nseasonal_order = (1, 1, 1, 12)  # Seasonal parameters\\n\\n# Fit SARIMA model\\nmodel = SARIMAX(endog, order=order, seasonal_order=seasonal_order)\\nresult = model.fit()\\n# 2. Split the Data\\ntrain_size = int(0.8 * len(df))  # 80% train, 20% test\\ntrain, test = df[:train_size], df[train_size:]\\n\\n# 3. Fit the SARIMA Model\\n# Define SARIMA parameters (p, d, q, P, D, Q, m)\\norder = (1, 1, 1)  # Non-seasonal parameters\\nseasonal_order = (1, 1, 1, 12)  # Seasonal parameters\\n\\n# Fit SARIMA model\\nmodel = SARIMAX(train, order=order, seasonal_order=seasonal_order)\\nresult = model.fit()\\n\\n# 4. Validate the Model\\n# Plot ACF and PACF of residuals\\nresiduals = result.resid\\nplot_acf(residuals)\\nplot_pacf(residuals)\\nplt.show()\\n\\n# 5. Forecast Future Values\\nforecast = result.forecast(steps=len(test))  # Forecast the test set period\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Assuming you have a time series dataset stored in a DataFrame called 'data'\n",
    "# with a datetime index\n",
    "# Example:\n",
    "# data = pd.read_csv('your_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# 1. Preprocess the Data\n",
    "# No missing values handling for simplicity\n",
    "# Example: data.dropna(inplace=True)\n",
    "\n",
    "# Assuming 'AQI' is the column you want to use as the endogenous variable\n",
    "endog = train['main_aqi']\n",
    "\n",
    "# Define SARIMA parameters\n",
    "order = (1, 1, 1)  # Non-seasonal parameters\n",
    "seasonal_order = (1, 1, 1, 12)  # Seasonal parameters\n",
    "\n",
    "# Fit SARIMA model\n",
    "model = SARIMAX(endog, order=order, seasonal_order=seasonal_order)\n",
    "result = model.fit()\n",
    "# 2. Split the Data\n",
    "train_size = int(0.8 * len(df))  # 80% train, 20% test\n",
    "train, test = df[:train_size], df[train_size:]\n",
    "\n",
    "# 3. Fit the SARIMA Model\n",
    "# Define SARIMA parameters (p, d, q, P, D, Q, m)\n",
    "order = (1, 1, 1)  # Non-seasonal parameters\n",
    "seasonal_order = (1, 1, 1, 12)  # Seasonal parameters\n",
    "\n",
    "# Fit SARIMA model\n",
    "model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
    "result = model.fit()\n",
    "\n",
    "# 4. Validate the Model\n",
    "# Plot ACF and PACF of residuals\n",
    "residuals = result.resid\n",
    "plot_acf(residuals)\n",
    "plot_pacf(residuals)\n",
    "plt.show()\n",
    "\n",
    "# 5. Forecast Future Values\n",
    "forecast = result.forecast(steps=len(test))  # Forecast the test set period'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
