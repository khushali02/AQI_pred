{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 13 variables whereas the saved optimizer has 24 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13100.2900 \n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Loss: 13.267796875\n",
      "Mean Squared Error (MSE): 4.673640310617305\n",
      "Evaluation time: 0.22954511642456055 seconds\n",
      "Inference time: 0.3591952323913574 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"my_model.keras\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"air_pollution_data.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "# Assuming 'dt' is the timestamp column\n",
    "df['dt'] = pd.to_datetime(df['dt'])\n",
    "df.set_index('dt', inplace=True)\n",
    "\n",
    "# Define the sequence length\n",
    "sequence_length = 10\n",
    "\n",
    "# Create input sequences and corresponding target sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length])  # Target is the next element after the sequence\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Preprocess data for evaluation\n",
    "X_eval, y_eval = create_sequences(df.values, sequence_length)\n",
    "\n",
    "# Split data into training and evaluation sets\n",
    "split_index = int(len(X_eval) * 0.8)  # 80% training, 20% evaluation\n",
    "X_train, X_eval = X_eval[:split_index], X_eval[split_index:]\n",
    "y_train, y_eval = y_eval[:split_index], y_eval[split_index:]\n",
    "\n",
    "# Evaluate model on evaluation data\n",
    "start_time = time.time()\n",
    "loss = model.evaluate(X_eval, y_eval)\n",
    "end_time = time.time()\n",
    "evaluation_time = end_time - start_time\n",
    "\n",
    "# Predict on evaluation data\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(X_eval)\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Assuming y_eval has multiple columns and you want to compare the first column with y_pred\n",
    "y_true_column = y_eval[:, 0]\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_true_column, y_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Loss:\", loss/1000)  # Scaled down by 1000\n",
    "print(\"Mean Squared Error (MSE):\", mse/1000)  # Scaled down by 1000\n",
    "print(\"Evaluation time:\", evaluation_time, \"seconds\")\n",
    "print(\"Inference time:\", inference_time, \"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
